# -*- coding: utf-8 -*-
"""ElecFinalProj_Grp4(REVISED).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DvZPBXYV54BVCURN3xqXuuifiuJXvVA2
"""

from google.colab import drive
drive.mount('/content/drive')

from pathlib import Path
import imghdr
import os

#image directory
#data_dir = "hair_types"
data_dir_train = "/content/drive/MyDrive/coffee_dataset/train"

image_extensions = [".png", ".jpg"]  # add there all your images file extensions

img_type_accepted_by_tf = ["bmp", "gif", "jpeg", "png"]
for filepath in Path(data_dir_train).rglob("*"):
    if filepath.suffix.lower() in image_extensions:
        img_type = imghdr.what(filepath)
        if img_type is None:
            print(f"{filepath} is not an image")
            os.remove(filepath)
        elif img_type not in img_type_accepted_by_tf:
            print(f"{filepath} is a {img_type}, not accepted by TensorFlow")
            os.remove(filepath)

data_dir_test = "/content/drive/MyDrive/coffee_dataset/test"
img_type_accepted_by_tf = ["bmp", "gif", "jpeg", "png"]
for filepath in Path(data_dir_test).rglob("*"):
    if filepath.suffix.lower() in image_extensions:
        img_type = imghdr.what(filepath)
        if img_type is None:
            print(f"{filepath} is not an image")
            os.remove(filepath)
        elif img_type not in img_type_accepted_by_tf:
            print(f"{filepath} is a {img_type}, not accepted by TensorFlow")
            os.remove(filepath)

#input selection
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

#added imports
from tensorflow.keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from IPython.display import Image

print(tf.__version__)

#CHANGES - seed, batchsize, image size, crop to aspect ratio

image_size = (224, 224) #resizing image to a linear basis
batch_size = 100 #number of images inputed into the network

test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_test,
    seed=1337,
    image_size=image_size,
    batch_size=batch_size,
    labels='inferred',
    label_mode='categorical',
    crop_to_aspect_ratio=True
)

#Input Selection of training dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    validation_split=0.1,
    subset="training",
    seed=1337,
    image_size=image_size,
    batch_size=batch_size,
    labels='inferred',
    label_mode='categorical',
    crop_to_aspect_ratio=True
)

#Input Selection of validation dataset
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    validation_split=0.1,
    subset="validation",
    seed=1337,
    image_size=image_size,
    batch_size=batch_size, 
    labels='inferred',
    label_mode='categorical',
    crop_to_aspect_ratio=True
)

#Visualizing data
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
      ax = plt.subplot(3, 3, i + 1)
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title(int(np.argmax(labels[i])))
      plt.axis("off")
plt.show()

"""# **Transfer Learning**
Frozen Resnet50 layers
*   Added dense (512, Activation = relu)
*   Added dropout (0.5)


"""

from keras.models import Sequential, Model
from keras.layers import Dense, Flatten, Dropout
from keras.applications import ResNet50, Xception
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization

resnet=Sequential()

# Resnet Layer ; Pretrained Model
pretrain = ResNet50(include_top=False,
                    input_shape=(image_size + (3,)),
                    classes=4,
                    weights='imagenet')
# Changed Value of Input layer

# resnet.add(layers.Rescaling(1.0 / 255)) #normalization function

pretrain.trainable = False

# Create new model on top
# inputs = keras.Input(shape=(image_size + (3,)))

# scale_layer = keras.layers.Rescaling(1/ 255)
# x = inputs
# x = scale_layer(x)
# x = pretrain(x, training=False)
# x = Flatten()(x)
# x = Dense(512, activation='relu')(x)
# x = Dense(256, activation='relu')(x)
# outputs = Dense(4, activation='softmax')(x)
# model = Model(inputs, outputs) 
# model.summary

for layer in pretrain.layers:
    layer.trainable=False
# dito yata tayo magdagdag ng changes for fine tuning

# Changed Output Layer

# start
resnet.add(keras.layers.Rescaling(1/ 255))
resnet.add(pretrain) #Resnet layer

# Sequential 
resnet.add(Flatten())
resnet.add(Dense(512, activation='relu'))
resnet.add(Dense(256, activation='relu'))
resnet.add(Dropout(0.5))
resnet.add(Dense(4,activation='softmax')) #based on number of classification
# # end

# tf.keras.utils.plot_model(resnet, to_file='model_test_resnet.png', show_shapes=True)
# resnet.summary()

print(pretrain.summary())

"""for x, layer in enumerate(pretrain.layers):
  print(x, layer.name, layer.trainable)
"""

from keras.metrics import Precision, Recall

epochs = 10
resnet.compile(optimizer=keras.optimizers.Adam(1e-5),loss="categorical_crossentropy",metrics=["accuracy"])
print("Layers Compiled")

# Train with freezed resnet
history = resnet.fit(train_ds,epochs=epochs,validation_data=test_ds)
print("Model Trained")

tf.keras.utils.plot_model(resnet, to_file='model_test_resnet.png', show_shapes=True)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""# Fine Tuning
*   Froze 0-164 layers
*   Unfroze 165 - 175 layers


"""

# Unfreeze Layers for Fine-Tuning
for layer in pretrain.layers[:161]: #0 - 170 layers freezed
  layer.trainable = False
for layer in pretrain.layers[161:]: #171 - 175 layers unfreezed
  layer.trainable = True

for x, layer in enumerate(pretrain.layers):
  print(x, layer.name, layer.trainable)

epochs = 50
learningRate = 1e-5
resnet.compile(optimizer=keras.optimizers.Adam(learningRate),loss="categorical_crossentropy",metrics=["accuracy"])
print("Layers Compiled")
print(resnet.summary())

# Train with freezed resnet
history = resnet.fit(train_ds,epochs=epochs,validation_data=test_ds)
print("Model Trained")

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# from sklearn.metrics import confusion_matrix
# import itertools

# labels = ['Dark', 'Green', 'Light', 'Medium']
# true = 

# cm = confusion_matrix(y_true=val_ds,
#                       y_pred=
#                       );

#@title Combine files
from os import listdir
from os.path import isfile, join

# Dark
darkPath = "/content/drive/MyDrive/coffee_dataset/test/Dark"
darkFiles = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(darkPath) for f in filenames]
darkFiles.sort()
# print(darkFiles)

# Green
greenPath = "/content/drive/MyDrive/coffee_dataset/test/Green"
greenFiles = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(greenPath) for f in filenames]
greenFiles.sort()
# print(greenFiles)

# Light
lightPath = "/content/drive/MyDrive/coffee_dataset/test/Light"
lightFiles = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(lightPath) for f in filenames]
lightFiles.sort()
# print(lightFiles)

# Medium
mediumPath = "/content/drive/MyDrive/coffee_dataset/test/Medium"
mediumFiles = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(mediumPath) for f in filenames]
mediumFiles.sort()
# print(mediumFiles)

# combine
coffeeFiles = darkFiles + greenFiles + lightFiles + mediumFiles
# print(coffeeFiles)
# print(len(coffeeFiles))

# # using test_ds
# img = keras.preprocessing.image.load_img(
#     "hair_types/Curly_Hair/02dac897d1dec9ba8c057a11d041ada8--layered-natural-hair-natural-black-hairstyles.jpg", target_size=image_size
# )
# img_array = keras.preprocessing.image.img_to_array(img)
# img_array = tf.expand_dims(img_array, 0)  # Create batch axis

# for images in test_ds:
  

# predictions = model.predict(img_array)
# print(
#     "This image is %.2f percent curly hair, %.2f percent straight hair, and %.2f percent wavy hair."
#     % tuple(predictions[0])
# )

# Commented out IPython magic to ensure Python compatibility.
# random image predictor

import random

randomNumber = random.randint(0, len(coffeeFiles)-1)
coffeePath = coffeeFiles[randomNumber]
print(coffeePath)
img = keras.preprocessing.image.load_img(
    str(coffeePath), target_size=image_size
)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)  # Create batch axis

predictions = resnet.predict(img_array)
print(
    "This image is %.2f percent dark, %.2f percent green, %.2f percent light, and %.2f percent medium."
#     % tuple(predictions[0])
)

# Commented out IPython magic to ensure Python compatibility.
# predict all files

for i in range(len(coffeeFiles)):
    print(i)
    coffeePath = coffeeFiles[i]
    print(coffeePath)
    img = keras.preprocessing.image.load_img(
        str(coffeePath), target_size=image_size
    )
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)  # Create batch axis

    predictions = resnet.predict(img_array)
    print(
        "This image is %.2f percent dark, %.2f percent green, %.2f percent light, and %.2f percent medium."
#         % tuple(predictions[0])
    )
    i+1

# Commented out IPython magic to ensure Python compatibility.
#@title Predict dark files (try lang)
# Predict dark files
 

for i in range(len(darkFiles)):
    print(i)
    eachDarkPath = darkFiles[i]
    print(eachDarkPath)
    img = keras.preprocessing.image.load_img(
        str(eachDarkPath), target_size=image_size
    )
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)  # Create batch axis

    predictions_dark = resnet.predict(img_array)
    print(
        "This image is %.2f percent dark, %.2f percent green, %.2f percent light, and %.2f percent medium."
#         % tuple(predictions_dark[0])
    )
    i+1

#@title (try ulit)
darkValues = []
greenValues = []
lightValues = []
mediumValues = []
y_predict = []  # try for confusion matrix

for i in range(len(coffeeFiles)):
    eachCoffeePath = coffeeFiles[i]
    print(str(i) + "\t"+ eachCoffeePath)
    img = keras.preprocessing.image.load_img(
        str(eachCoffeePath), target_size=image_size
    )
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)  # Create batch axis

    predictions_coffee = resnet.predict(img_array)
    tupleCoffee = tuple(predictions_coffee[0])
    listCoffee = list(tupleCoffee)
    roundListCoffee = [round(item, 4) for item in listCoffee]
    if(i<=99):
        darkValues.append(roundListCoffee[0])
    elif(99<i<=199):
        greenValues.append(roundListCoffee[1])
    elif(199<i<=299):
        lightValues.append(roundListCoffee[2])
    elif(299<i<=399):
        mediumValues.append(roundListCoffee[3])
    
    # y_predict.append(roundListCoffee.index(max(roundListCoffee)))  # try for confusion matrix
  
    print("Image is " + str(np.round(roundListCoffee[0]*100, decimals=4)) + "% dark, " + str(np.round(roundListCoffee[1]*100, decimals=4)) + "% green, " + 
          str(np.round(roundListCoffee[2]*100, decimals=4)) + "% light, and " + str(np.round(roundListCoffee[3]*100, decimals=4)) + "% medium." + "\n")
    i+1

#@title Mean ng green predictions sa green class (not sure kung magagamit)
from statistics import mean 

# print(darkValues)
print("Mean green predictions for dark Images: " + str(round(mean(darkValues),4)))
print("Minimum dark: " + str(min(darkValues)))
print("Minimum at Index " + str(darkValues.index(min(darkValues))))

print("Mean green predictions for Green Images: " + str(round(mean(greenValues),4)))
print("Minimum green: " + str(min(greenValues)))
print("Minimum at Index " + str(greenValues.index(min(greenValues))))

print("Mean light predictions for light Images: " + str(round(mean(lightValues),4)))
print("Minimum light: " + str(min(lightValues)))
print("Minimum at Index " + str(lightValues.index(min(lightValues))))

print("Mean medium predictions for medium Images: " + str(round(mean(mediumValues),4)))
print("Minimum medium: " + str(min(mediumValues)))
print("Minimum at Index " + str(mediumValues.index(min(mediumValues))))

